{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 tutorial을 참고해서 IMDB 무비 리뷰 \n",
    "\n",
    "* https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_trains = []\n",
    "review_tests = []\n",
    "for line in open('./movie_data/full_train.txt', 'r'):\n",
    "    review_trains.append(line.strip())\n",
    "for line in open('./movie_data/full_test.txt', 'r'):\n",
    "    review_tests.append(line.strip())\n",
    "review_trains[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clutter token 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "review_trains_clean = preprocess_reviews(review_trains)    \n",
    "review_tests_clean = preprocess_reviews(review_tests)\n",
    "review_trains_clean[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seonghoonjung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "a few example of stop words = [\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(f'a few example of stop words = {stop_words[10:20]}')\n",
    "\n",
    "def remove_stop_words(corpus, stop_words):\n",
    "    results = []\n",
    "    for review in corpus:\n",
    "        results.append(' '.join([word for word in review.split() if word not in stop_words]))\n",
    "    return results\n",
    "\n",
    "review_trains_no_stopword = remove_stop_words(review_trains_clean, stop_words)\n",
    "review_tests_no_stopword = remove_stop_words(review_tests_clean, stop_words)\n",
    "review_trains_no_stopword[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (take times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:10<00:00, 355.44it/s]\n",
      "100%|██████████| 25000/25000 [01:08<00:00, 367.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bromwel high cartoon comedi ran time program school life teacher 35 year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import *\n",
    "def get_stemmed(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    results = []\n",
    "    for review in tqdm(corpus):\n",
    "        results.append(' '.join([stemmer.stem(word) for word in review.split()]))\n",
    "    return results     \n",
    "\n",
    "review_trains_stemmed = get_stemmed(review_trains_no_stopword)\n",
    "review_tests_stemmed = get_stemmed(review_tests_no_stopword)\n",
    "review_trains_stemmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "* Lemmatization works by identifying the part-of-speech of a given word and then applying more complex rules to transform the word into its true root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/seonghoonjung/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:15<00:00, 1654.62it/s]\n",
      "100%|██████████| 25000/25000 [00:13<00:00, 1874.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bromwel high cartoon comedi ran time program school life teacher 35 year teach profess lead believ bromwel high satir much closer realiti teacher scrambl surviv financi insight student see right pathet teacher pomp petti whole situat remind school knew student saw episod student repeatedli tri burn school immedi recal high classic line inspector im sack one teacher student welcom bromwel high expect mani adult age think bromwel high far fetch piti isnt'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_lemmatized(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    results = []\n",
    "    for review in tqdm(corpus):\n",
    "        results.append(' '.join([lemmatizer.lemmatize(word) for word in review.split()]))\n",
    "    return results  \n",
    "\n",
    "review_trains_lemma = get_lemmatized(review_trains_stemmed)\n",
    "review_tests_lemma = get_lemmatized(review_tests_stemmed)\n",
    "review_trains_lemma[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: One-hot encoding, 2-gram, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca size=1636674\n",
      "some voca=['00', '00 10', '00 1991', '00 doc', '00 either', '00 far', '00 howev', '00 keoni', '00 seem', '00 seen']\n",
      "(25000, 1636674)\n",
      "Accuracy for C=0.01: 0.87968\n",
      "Accuracy for C=0.05: 0.88448\n",
      "Accuracy for C=0.25: 0.8856\n",
      "Accuracy for C=0.5: 0.88432\n",
      "Accuracy for C=1: 0.88544\n",
      "Accuracy = 0.88432\n",
      "most influential words for positive sentiments\n",
      "[('excel', 1.3943030663296845), ('perfect', 1.2132720628578177), ('must see', 0.9594619622451207), ('favorit', 0.91086838943874), ('superb', 0.8974273984498831)]\n",
      "most influential words for negative sentiments\n",
      "[('worst', -1.8627433646673672), ('aw', -1.5114004350012715), ('wast', -1.3488624224513295), ('bore', -1.3252523670630436), ('disappoint', -1.1635156302010716)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "######### n-gram vectorization\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngrame_vectorizer = CountVectorizer(binary=True, # count가 아닌 occurance or not\n",
    "                                    stop_words=stop_words,\n",
    "                                    ngram_range=[1,2]) # bi-gram까지 사용\n",
    "ngrame_vectorizer.fit(review_trains_lemma)\n",
    "train_features = ngrame_vectorizer.transform(review_trains_lemma)\n",
    "test_features = ngrame_vectorizer.transform(review_tests_lemma)\n",
    "print(f'voca size={len(ngrame_vectorizer.get_feature_names())}')\n",
    "print(f'some voca={ngrame_vectorizer.get_feature_names()[:10]}')\n",
    "print(train_features.toarray().shape)\n",
    "\n",
    "########### learn by logistic regression\n",
    "\n",
    "# label 생성 - 초반 절반은 1, 나머지는 0\n",
    "labels = [1 if i < 12500 else 0 for i in range(25000)]  \n",
    "\n",
    "# train/validation set 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, labels, test_size=0.25, shuffle=True)\n",
    "\n",
    "# Logistic regression으로 학습 (다양한 regularizer, small value mean strong regularization)\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "# 최종 모델은 위에서 C=0.5 일 때를 사용\n",
    "final_model = LogisticRegression(C=0.5)\n",
    "final_model.fit(X_train, y_train)\n",
    "print (\"Accuracy = %s\" % (accuracy_score(y_val, final_model.predict(X_val))))\n",
    "final_model.coef_[0].shape\n",
    "\n",
    "\n",
    "######## XAI\n",
    "feature_to_weight_map = {\n",
    "    word: weight for word, weight in zip(ngrame_vectorizer.get_feature_names(), final_model.coef_[0])\n",
    "}\n",
    "\n",
    "print('most influential words for positive sentiments')\n",
    "print(sorted(feature_to_weight_map.items(), key=lambda x:x[1], reverse=True)[:5])\n",
    "print('most influential words for negative sentiments')\n",
    "print(sorted(feature_to_weight_map.items(), key=lambda x:x[1], reverse=False)[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: TF-IDF, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca size=65101\n",
      "some voca=['00', '000', '0000000000001', '000001', '0001', '00015', '001', '002', '003830', '006']\n",
      "(25000, 65101)\n",
      "Accuracy for C=0.01: 0.82864\n",
      "Accuracy for C=0.05: 0.8528\n",
      "Accuracy for C=0.25: 0.8776\n",
      "Accuracy for C=0.5: 0.88288\n",
      "Accuracy for C=1: 0.88832\n",
      "Accuracy = 0.88832\n",
      "most influential words for positive sentiments\n",
      "[('great', 6.607355574976493), ('excel', 5.778575171737329), ('love', 4.910504257864102), ('enjoy', 4.627837667692939), ('best', 4.587195439977589)]\n",
      "most influential words for negative sentiments\n",
      "[('worst', -8.019276948167178), ('bad', -7.305807963722026), ('wast', -6.438358493101818), ('aw', -5.963015244318026), ('bore', -5.760169345750094)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "######### tf-idf vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(review_trains_lemma)\n",
    "train_features = tfidf_vectorizer.transform(review_trains_lemma)\n",
    "test_features = tfidf_vectorizer.transform(review_tests_lemma)\n",
    "print(f'voca size={len(tfidf_vectorizer.get_feature_names())}')\n",
    "print(f'some voca={tfidf_vectorizer.get_feature_names()[:10]}')\n",
    "print(train_features.toarray().shape)\n",
    "\n",
    "########### learn by logistic regression\n",
    "\n",
    "# label 생성 - 초반 절반은 1, 나머지는 0\n",
    "labels = [1 if i < 12500 else 0 for i in range(25000)]  \n",
    "\n",
    "# train/validation set 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, labels, test_size=0.25, shuffle=True)\n",
    "\n",
    "# Logistic regression으로 학습 (다양한 regularizer, small value mean strong regularization)\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "# 최종 모델은 위에서 C=1 일 때를 사용\n",
    "final_model = LogisticRegression(C=1)\n",
    "final_model.fit(X_train, y_train)\n",
    "print (\"Accuracy = %s\" % (accuracy_score(y_val, final_model.predict(X_val))))\n",
    "final_model.coef_[0].shape\n",
    "\n",
    "\n",
    "######## XAI\n",
    "feature_to_weight_map = {\n",
    "    word: weight for word, weight in zip(tfidf_vectorizer.get_feature_names(), final_model.coef_[0])\n",
    "}\n",
    "\n",
    "print('most influential words for positive sentiments')\n",
    "print(sorted(feature_to_weight_map.items(), key=lambda x:x[1], reverse=True)[:5])\n",
    "print('most influential words for negative sentiments')\n",
    "print(sorted(feature_to_weight_map.items(), key=lambda x:x[1], reverse=False)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
