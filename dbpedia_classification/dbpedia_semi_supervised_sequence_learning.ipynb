{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 출처 : https://github.com/dongjun-Lee/transfer-learning-text-tf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 및 각종 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/seonghoonjung/anaconda3/lib/python3.6/site-packages (3.2)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seonghoonjung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wget\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 20\n",
    "EMBEDDING_SIZE = 256\n",
    "HIDDEN_UNITS = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 2\n",
    "TRAIN_PATH = \"dbpedia_csv/train.csv\"\n",
    "TEST_PATH = \"dbpedia_csvtest.csv\"\n",
    "WORD_DICT_PATH = \"word_dict.pickle\"\n",
    "TRAIN_PERCENT = 0.2\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 유틸리티 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     15,
     21,
     27,
     57
    ]
   },
   "outputs": [],
   "source": [
    "def download_dbpedia():\n",
    "    dbpedia_url = 'https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz'\n",
    "\n",
    "    wget.download(dbpedia_url)\n",
    "    with tarfile.open(\"dbpedia_csv.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "def clean_str(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text\n",
    "def build_word_dict():\n",
    "    if not os.path.exists(WORD_DICT_PATH):\n",
    "        train_df = pd.read_csv(TRAIN_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        contents = train_df[\"content\"]\n",
    "\n",
    "        words = list()\n",
    "        for content in contents:\n",
    "            for word in word_tokenize(clean_str(content)):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<pad>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, count in word_counter:\n",
    "            if count > 1:\n",
    "                word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(WORD_DICT_PATH, \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    else:\n",
    "        with open(WORD_DICT_PATH, \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "            \n",
    "    dict_word = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "    return word_dict,dict_word \n",
    "def build_word_dataset(step, frac, word_dict, document_max_len):\n",
    "    if step == \"train\":\n",
    "        df = pd.read_csv(TRAIN_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        # Shuffle dataframe\n",
    "        df = df.sample(frac=frac)\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        df = df.sample(frac=1.0)\n",
    "        \n",
    "    x = list(map(lambda d: word_tokenize(clean_str(d)), df[\"content\"]))\n",
    "    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
    "    x = list(map(lambda d: d[:document_max_len], x))\n",
    "    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
    "\n",
    "    y = list(map(lambda d: d - 1, list(df[\"class\"])))\n",
    "\n",
    "    return x, y\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word dict\n",
      "Build word dataset\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('dbpedia_csv'):\n",
    "    print(\"Downloading dbpedia dataset...\")\n",
    "    download_dbpedia()\n",
    "    \n",
    "print('Build word dict')\n",
    "word_dict, dict_word = build_word_dict()\n",
    "\n",
    "print('Build word dataset')\n",
    "train_x, train_y = build_word_dataset(\"train\", TRAIN_PERCENT, word_dict, MAX_DOCUMENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kruszyn kraje ski kru n kra j ski ( german deutsch <unk> ) is a village in the administrative district -> 8\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_idx = 0\n",
    "sentence = ' '.join([dict_word[idx] for idx in train_x[sample_sentence_idx]]) \n",
    "print(f'{sentence} -> {train_y[sample_sentence_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a dinner => <s> I have a dinner </s>\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, word_dict, max_length, embedding_size, units, batch_size):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.voca_size = len(word_dict)\n",
    "        self.units = units\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, [None, max_length])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.decoder_input = tf.concat([tf.ones([self.batch_size, 1], tf.int32)*word_dict['<s>'],self.x], axis=1)\n",
    "        self.decoder_output = tf.concat([self.x, tf.ones([self.batch_size,1], tf.int32)*word_dict['</s>']], axis=1)\n",
    "        \n",
    "        self.encoder_input_len = tf.reduce_sum(tf.sign(self.x),1)\n",
    "        self.decoder_input_len = tf.reduce_sum(tf.sign(self.decoder_input),1)\n",
    "        \n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            init_embeddings = tf.random_uniform([self.voca_size, self.embedding_size])\n",
    "            embeddings = tf.get_variable(\"embedding\", initializer=init_embeddings)\n",
    "            encoder_input_emb = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "            decoder_input_emb = tf.nn.embedding_lookup(embeddings, self.decoder_input)\n",
    "        \n",
    "        with tf.variable_scope(\"rnn\"):\n",
    "            encoder_cell = tf.contrib.rnn.BasicLSTMCell(self.units)\n",
    "            _, encoder_states = tf.nn.dynamic_rnn(encoder_cell, encoder_input_emb, \n",
    "                                                  sequence_length=self.encoder_input_len,\n",
    "                                                  dtype=tf.float32)\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            decoder_cell = tf.contrib.rnn.BasicLSTMCell(self.units)\n",
    "            # shape of decoder outputs  = [batch_size, sequence_length, units]\n",
    "            decoder_outputs, _ = tf.nn.dynamic_rnn(decoder_cell, decoder_input_emb,\n",
    "                                                   sequence_length=self.decoder_input_len,\n",
    "                                                   dtype=tf.float32)\n",
    "            \n",
    "        with tf.name_scope(\"output\"):\n",
    "            # shape = [batch_size, sequence_length, voca_size]\n",
    "            self.logits = tf.layers.dense(decoder_outputs, self.voca_size)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # shape = [sequence_length, ]\n",
    "            losses = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits = self.logits,\n",
    "                targets = self.decoder_output,\n",
    "                weights = tf.sequence_mask(self.decoder_input_len, max_length+1,dtype=tf.float32),\n",
    "                average_across_timesteps=False,\n",
    "                average_across_batch=True)\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = AutoEncoder(word_dict, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE, HIDDEN_UNITS, BATCH_SIZE)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder를 이용한 사전학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_SAVE_PATH = \"pretrain_logs\"\n",
    "def pre_train(model, train_x, train_y, word_dict):\n",
    "    with tf.Session() as sess:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(model.loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
    "        \n",
    "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter(PRETRAIN_SAVE_PATH, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(batch_x):\n",
    "            feed_dict = {model.x: batch_x}\n",
    "            _, step, summaries, loss = sess.run([train_op, global_step, summary_op, model.loss],\n",
    "                                                feed_dict = feed_dict\n",
    "                                               )\n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            return loss\n",
    "  \n",
    "        batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n",
    "        total_step = ( len(train_x) / BATCH_SIZE ) * NUM_EPOCHS\n",
    "        for batch_x,_ in batches:\n",
    "            loss = train_step(batch_x)\n",
    "            step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if step == 1 or step % 20 == 0:\n",
    "                print(f'{step}/{total_step}, loss={loss}')\n",
    "            if step % 100 == 0:\n",
    "                saver.save(sess, os.path.join(PRETRAIN_SAVE_PATH, \"model\", \"model.ckpt\"), global_step=step)\n",
    "\n",
    "pre_train(model, train_x, train_y, word_dict)            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
