{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 출처 : https://github.com/dongjun-Lee/transfer-learning-text-tf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 및 각종 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/seonghoonjung/anaconda3/lib/python3.6/site-packages (3.2)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seonghoonjung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wget\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 20\n",
    "EMBEDDING_SIZE = 256\n",
    "HIDDEN_UNITS = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CLASS = 14\n",
    "TRAIN_PATH = \"dbpedia_csv/train.csv\"\n",
    "TEST_PATH = \"dbpedia_csv/test.csv\"\n",
    "WORD_DICT_PATH = \"word_dict.pickle\"\n",
    "TRAIN_PERCENT = 0.2\n",
    "PRETRAIN_SAVE_PATH = \"pretrain_logs\"\n",
    "TRAIN_SAVE_PATH = \"train_logs\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 유틸리티 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     6,
     12,
     42,
     59
    ]
   },
   "outputs": [],
   "source": [
    "def download_dbpedia():\n",
    "    dbpedia_url = 'https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz'\n",
    "\n",
    "    wget.download(dbpedia_url)\n",
    "    with tarfile.open(\"dbpedia_csv.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "def clean_str(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text\n",
    "def build_word_dict():\n",
    "    if not os.path.exists(WORD_DICT_PATH):\n",
    "        train_df = pd.read_csv(TRAIN_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        contents = train_df[\"content\"]\n",
    "\n",
    "        words = list()\n",
    "        for content in contents:\n",
    "            for word in word_tokenize(clean_str(content)):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<pad>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, count in word_counter:\n",
    "            if count > 1:\n",
    "                word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(WORD_DICT_PATH, \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    else:\n",
    "        with open(WORD_DICT_PATH, \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "            \n",
    "    dict_word = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "    return word_dict,dict_word \n",
    "def build_word_dataset(step, frac, word_dict, document_max_len):\n",
    "    if step == \"train\":\n",
    "        df = pd.read_csv(TRAIN_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        # Shuffle dataframe\n",
    "        df = df.sample(frac=frac)\n",
    "    else:\n",
    "        df = pd.read_csv(TEST_PATH, names=[\"class\", \"title\", \"content\"])\n",
    "        df = df.sample(frac=1.0)\n",
    "        \n",
    "    x = list(map(lambda d: word_tokenize(clean_str(d)), df[\"content\"]))\n",
    "    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
    "    x = list(map(lambda d: d[:document_max_len], x))\n",
    "    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
    "\n",
    "    y = list(map(lambda d: d - 1, list(df[\"class\"])))\n",
    "\n",
    "    return x, y\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word dict\n",
      "Build word dataset\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('dbpedia_csv'):\n",
    "    print(\"Downloading dbpedia dataset...\")\n",
    "    download_dbpedia()\n",
    "    \n",
    "print('Build word dict')\n",
    "word_dict, dict_word = build_word_dict()\n",
    "\n",
    "print('Build word dataset')\n",
    "train_x, train_y = build_word_dataset(\"train\", TRAIN_PERCENT, word_dict, MAX_DOCUMENT_LENGTH)\n",
    "test_x, test_y = build_word_dataset(\"test\", TRAIN_PERCENT, word_dict, MAX_DOCUMENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnen is a village in the municipality haren groningen in the netherlands there are some picturesque farms and windmills in -> 8\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_idx = 0\n",
    "sentence = ' '.join([dict_word[idx] for idx in train_x[sample_sentence_idx]]) \n",
    "print(f'{sentence} -> {train_y[sample_sentence_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model for dbpedia classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNN(object):\n",
    "    def __init__(self, voca_size, embedding_size, units, max_length, num_class):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.units = units\n",
    "        self.fc_units = 256\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, [None, max_length])\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        \n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            init_embeddings = tf.random.uniform([voca_size, self.embedding_size])\n",
    "            embeddings = tf.get_variable(\"embedding\", initializer=init_embeddings)\n",
    "            x_emb = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        \n",
    "        with tf.variable_scope(\"rnn\"):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.units)\n",
    "            rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "                cell, x_emb, sequence_length=self.x_len, dtype=tf.float32\n",
    "            )\n",
    "            rnn_output_flat = tf.reshape(rnn_outputs, [-1, max_length*self.units])\n",
    "            \n",
    "        with tf.name_scope(\"fc\"):\n",
    "            fc_output = tf.layers.dense(rnn_output_flat, self.fc_units, activation=tf.nn.relu)\n",
    "            dropout = tf.nn.dropout(fc_output, self.keep_prob)\n",
    "            \n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.layers.dense(dropout, num_class)\n",
    "            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y)\n",
    "            )\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = WordRNN(len(word_dict), EMBEDDING_SIZE, 128, MAX_DOCUMENT_LENGTH, NUM_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이진 분류 학습 함수 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'embedding/embedding:0' shape=(268964, 256) dtype=float32_ref>]\n",
      "INFO:tensorflow:Restoring parameters from pretrain_logs/model/model.ckpt-100\n",
      "test_accuracy = 0.09185714285714286\n",
      "\n",
      "step 100 : loss = 2.7442877292633057\n",
      "step 200 : loss = 1.6490721702575684\n",
      "test_accuracy = 0.43442857142857144\n",
      "\n",
      "step 300 : loss = 1.5403660535812378\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_x, train_y, test_x, test_y, vocabulary_size):\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Define training procedure\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(model.loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
    "\n",
    "        # Summary\n",
    "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter(TRAIN_SAVE_PATH, sess.graph)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Load variables from pre-trained model\n",
    "        pre_trained_variables = [v for v in tf.global_variables()\n",
    "                                 if (v.name.startswith(\"embedding\") or v.name.startswith(\"birnn\")) and \"Adam\" not in v.name]\n",
    "        print(pre_trained_variables)\n",
    "        saver = tf.train.Saver(pre_trained_variables)\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.join(PRETRAIN_SAVE_PATH, \"model\"))\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        def train_step(batch_x, batch_y):\n",
    "            feed_dict = {\n",
    "                model.x: batch_x,\n",
    "                model.y: batch_y,\n",
    "                model.keep_prob: 0.5\n",
    "            }\n",
    "\n",
    "            _, step, summaries, loss = sess.run([train_op, global_step, summary_op, model.loss], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\"step {0} : loss = {1}\".format(step, loss))\n",
    "\n",
    "        def test_accuracy(test_x, test_y):\n",
    "            test_batches = batch_iter(test_x, test_y, BATCH_SIZE, 1)\n",
    "            sum_accuracy, cnt = 0, 0\n",
    "\n",
    "            for test_batch_x, test_batch_y in test_batches:\n",
    "                accuracy = sess.run(model.accuracy, feed_dict={model.x: test_batch_x, model.y: test_batch_y, model.keep_prob: 1.0})\n",
    "                sum_accuracy += accuracy\n",
    "                cnt += 1\n",
    "\n",
    "            with open(TRAIN_SAVE_PATH +\"-accuracy.txt\", \"a\") as f:\n",
    "                print(sum_accuracy/cnt, file=f)\n",
    "\n",
    "            return sum_accuracy / cnt\n",
    "\n",
    "        # Training loop\n",
    "        batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n",
    "\n",
    "        for batch_x, batch_y in batches:\n",
    "            train_step(batch_x, batch_y)\n",
    "            step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if step == 1 or step % 200 == 0:\n",
    "                test_acc = test_accuracy(test_x, test_y)\n",
    "                print(\"test_accuracy = {0}\\n\".format(test_acc))\n",
    "train(model, train_x, train_y, test_x, test_y, len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Variable', []),\n",
       " ('Variable_1', []),\n",
       " ('beta1_power', []),\n",
       " ('beta1_power_1', []),\n",
       " ('beta2_power', []),\n",
       " ('beta2_power_1', []),\n",
       " ('decoder/rnn/basic_lstm_cell/bias', [512]),\n",
       " ('decoder/rnn/basic_lstm_cell/bias/Adam', [512]),\n",
       " ('decoder/rnn/basic_lstm_cell/bias/Adam_1', [512]),\n",
       " ('decoder/rnn/basic_lstm_cell/bias/Adam_2', [512]),\n",
       " ('decoder/rnn/basic_lstm_cell/bias/Adam_3', [512]),\n",
       " ('decoder/rnn/basic_lstm_cell/kernel', [384, 512]),\n",
       " ('decoder/rnn/basic_lstm_cell/kernel/Adam', [384, 512]),\n",
       " ('decoder/rnn/basic_lstm_cell/kernel/Adam_1', [384, 512]),\n",
       " ('decoder/rnn/basic_lstm_cell/kernel/Adam_2', [384, 512]),\n",
       " ('decoder/rnn/basic_lstm_cell/kernel/Adam_3', [384, 512]),\n",
       " ('dense/bias', [268964]),\n",
       " ('dense/bias/Adam', [268964]),\n",
       " ('dense/bias/Adam_1', [268964]),\n",
       " ('dense/bias/Adam_2', [268964]),\n",
       " ('dense/bias/Adam_3', [268964]),\n",
       " ('dense/kernel', [128, 268964]),\n",
       " ('dense/kernel/Adam', [128, 268964]),\n",
       " ('dense/kernel/Adam_1', [128, 268964]),\n",
       " ('dense/kernel/Adam_2', [128, 268964]),\n",
       " ('dense/kernel/Adam_3', [128, 268964]),\n",
       " ('embedding/embedding', [268964, 256]),\n",
       " ('embedding/embedding/Adam', [268964, 256]),\n",
       " ('embedding/embedding/Adam_1', [268964, 256]),\n",
       " ('embedding/embedding/Adam_2', [268964, 256]),\n",
       " ('embedding/embedding/Adam_3', [268964, 256]),\n",
       " ('rnn/rnn/basic_lstm_cell/bias', [512]),\n",
       " ('rnn/rnn/basic_lstm_cell/kernel', [384, 512])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_list = tf.train.list_variables(os.path.join(PRETRAIN_SAVE_PATH, \"model\")) \n",
    "inspect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
